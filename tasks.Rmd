---
title: 'Lab assignment 3'
author: ""
output:
  html_document:
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---

## Work breakdown:

#### Next, generate the messages

# Part I: Parameter estimation

## Task 1

We are estimating the parameter Œ∏ (where mean Œº=Œ∏) of an Exponential
distribution. We will test four different methods for constructing
confidence intervals by running simulations. We will vary the sample
size n and confidence level (1‚àíŒ±) to see how often each interval
actually captures the true parameter (Coverage Probability) and how wide
the intervals are (Precision). Our team id number is 23.

```{r}
# SETUP

library(ggplot2)

id_num <- 23
theta <- id_num / 10    
lambda <- 1 / theta     
m <- 10000             

alphas <- c(0.1, 0.05, 0.01)
n_sizes <- c(10, 50, 100, 1000)

results <- data.frame()

# Helper function to calculate coverage and length
calc_metrics <- function(L, U, true_param) {
  cov <- mean((true_param >= L) & (true_param <= U))
  len <- mean(U - L)
  return(c(cov, len))
}

print(paste("Simulation initialized for Theta =", theta))
```

### 1. Method 1: Exact Distribution (Chi-Squared)

This method utilizes the property that the sum of independent
exponential variables follows a Gamma distribution. Specifically, the
sample mean $\overline{X}$ and the parameter $\theta$ can be expressed
with a Chi-squared distribution with $2n$ degrees of freedom.

$$
\quad \frac{2n\overline{X}}{\theta} \sim \chi^2_{2n}
$$

To find the $1-\alpha$ confidence interval, we set up:

$$
P\left( \chi^2_{\alpha/2, 2n} \le \frac{2n\overline{X}}{\theta} \le \chi^2_{1-\alpha/2, 2n} \right) = 1 - \alpha
$$

By inverting the inequality to isolate $\theta$, we obtain the
confidence interval:

$$
\left[ \frac{2n\overline{X}}{\chi^2_{1-\alpha/2, 2n}}, \quad \frac{2n\overline{X}}{\chi^2_{\alpha/2, 2n}} \right]
$$

```{r}
# METHOD 1: EXACT (CHI-SQUARED)
set.seed(2025) 

for (n in n_sizes) {
  # Generate data (rows = sims, cols = sample size)
  data <- matrix(rexp(n * m, rate = lambda), nrow = m, ncol = n)
  x_bar <- rowMeans(data)
  
  for (alpha in alphas) {
    # Chi-square Quantiles
    q_low <- qchisq(alpha/2, df = 2*n)
    q_high <- qchisq(1 - alpha/2, df = 2*n)
    
    # Interval Calculation
    L <- (2 * n * x_bar) / q_high
    U <- (2 * n * x_bar) / q_low
    
    # Evaluate
    stats <- calc_metrics(L, U, theta)
    results <- rbind(results, data.frame(n=n, alpha=alpha, Method="1. Exact", 
                                         Cov=stats[1], Len=stats[2]))
  }
}
```

### 2. Method 2: Normal Approximation (Known Variance)

This method assumes the Central Limit Theorem holds and calculates the
interval as if the population variance ($\sigma^2 = \theta^2$) were
known.

$$
Z = \frac{\overline{X} - \theta}{\theta/\sqrt{n}} \sim \mathcal{N}(0, 1)
$$

The resulting confidence interval is:

$$
\left[ \overline{X} - z_{1-\alpha/2}\frac{\theta}{\sqrt{n}}, \quad \overline{X} + z_{1-\alpha/2}\frac{\theta}{\sqrt{n}} \right]
$$

```{r}
# METHOD 2: NORMAL (KNOWN THETA)
set.seed(2025) 

for (n in n_sizes) {
  data <- matrix(rexp(n * m, rate = lambda), nrow = m, ncol = n)
  x_bar <- rowMeans(data)
  
  for (alpha in alphas) {
    z <- qnorm(1 - alpha/2)
    
    margin <- z * (theta / sqrt(n))
    L <- x_bar - margin
    U <- x_bar + margin
    
    stats <- calc_metrics(L, U, theta)
    results <- rbind(results, data.frame(n=n, alpha=alpha, Method="2. Norm(Known)", 
                                         Cov=stats[1], Len=stats[2]))
  }
}
```

### 3. Method 3: Normal Approximation

To remove the dependence on the unknown $\theta$ in the standard error
term of Method 2, we start with the inequality:

$$
|\overline{X} - \theta| \le z_{1-\alpha/2} \frac{\theta}{\sqrt{n}}
$$

Let $k = \frac{z_{1-\alpha/2}}{\sqrt{n}}$. We solve for $\theta$:

$$
\begin{aligned}
-k\theta &\le \overline{X} - \theta \le k\theta \\
\theta - k\theta &\le \overline{X} \le \theta + k\theta \\
\theta(1 - k) &\le \overline{X} \le \theta(1 + k)
\end{aligned}
$$

Inverting for $\theta$ gives the interval:

$$
\left[ \frac{\overline{X}}{1 + \frac{z_{1-\alpha/2}}{\sqrt{n}}}, \quad \frac{\overline{X}}{1 - \frac{z_{1-\alpha/2}}{\sqrt{n}}} \right]
$$

```{r}
# METHOD 3: NORMAL (SOLVED INEQUALITY)
set.seed(2025)

for (n in n_sizes) {
  data <- matrix(rexp(n * m, rate = lambda), nrow = m, ncol = n)
  x_bar <- rowMeans(data)
  
  for (alpha in alphas) {
    z <- qnorm(1 - alpha/2)
    k <- z / sqrt(n)
    
    # Interval Calculation
    L <- x_bar / (1 + k)
    U <- x_bar / (1 - k)
    
    stats <- calc_metrics(L, U, theta)
    results <- rbind(results, data.frame(n=n, alpha=alpha, Method="3. Norm(Solved)", 
                                         Cov=stats[1], Len=stats[2]))
  }
}
```

### 4. Method 4: Student's t-distribution 

This is the standard asymptotic approach when the variance is unknown.
We estimate the population standard deviation $\sigma$ using the sample
standard deviation $s$.

$$
T = \frac{\overline{X} - \theta}{s/\sqrt{n}} \sim t_{n-1}
$$

The confidence interval is:

$$
\left[ \overline{X} - t_{n-1, 1-\alpha/2}\frac{s}{\sqrt{n}}, \quad \overline{X} + t_{n-1, 1-\alpha/2}\frac{s}{\sqrt{n}} \right]
$$

```{r}
# METHOD 4: STUDENT'S T
set.seed(2025)

for (n in n_sizes) {
  data <- matrix(rexp(n * m, rate = lambda), nrow = m, ncol = n)
  x_bar <- rowMeans(data)
  s_sd <- apply(data, 1, sd)
  
  for (alpha in alphas) {
    t_val <- qt(1 - alpha/2, df = n - 1)
    
    # Interval Calculation
    margin <- t_val * (s_sd / sqrt(n))
    L <- x_bar - margin
    U <- x_bar + margin
    
    stats <- calc_metrics(L, U, theta)
    results <- rbind(results, data.frame(n=n, alpha=alpha, Method="4. Student-t", 
                                         Cov=stats[1], Len=stats[2]))
  }
}
```

### Results Comparison

```{r}
library(knitr)

colnames(results)[which(names(results) %in% c("Coverage", "Cov"))] <- "Cov"
colnames(results)[which(names(results) %in% c("AvgLength", "Length", "Len"))] <- "Len"

clean_results <- results
colnames(clean_results)[which(names(clean_results) %in% c("Coverage", "Cov"))] <- "Cov"
colnames(clean_results)[which(names(clean_results) %in% c("AvgLength", "Length", "Len"))] <- "Len"

if (!requireNamespace("knitr", quietly = TRUE)) install.packages("knitr")
library(knitr)

options(max.print = 100000)

results_sorted <- clean_results[order(clean_results$n, clean_results$Method), ]

unique_alphas <- sort(unique(results_sorted$alpha), decreasing = TRUE)

for (curr_alpha in unique_alphas) {

  conf_level <- (1 - curr_alpha) * 100

  sub_table <- subset(results_sorted, alpha == curr_alpha)

  rownames(sub_table) <- NULL

  cat("\n\n")
  cat(paste("### Results for Alpha =", curr_alpha, "(", conf_level, "% Confidence )"))
  cat("\n")

  print(kable(sub_table, digits = 4, row.names = FALSE))
}
```

### Lengths & Precision comparison

Precision is measured by the average length of the confidence interval
(column `Len`). A shorter interval indicates higher precision, provided
the coverage probability remains valid.

1.  **Effect of Sample Size (**$n$): Across all methods and alpha
    levels, the length decreases as sample size increases. This follows
    the $1/\sqrt{n}$ . For example, at $\alpha=0.05$, Method 1's length
    drops from **3.44** ($n=10$) to **0.28** ($n=1000$).

2.  **Comparing Methods (**$n=10$):

    -   **Method 2 (Normal, Known** $\theta$): Consistently produces the
        shortest (most precise) intervals (e.g., Len = 2.39 at
        $\alpha=0.1$). However, this is a theoretical benchmark that is
        impossible to achieve in practice because it requires knowing
        the true parameter $\theta$.
    -   **Method 4 (Student's t):** Produces the second shortest
        intervals (e.g., Len = 2.46 at $\alpha=0.1$). However, looking
        at the Coverage column (`Cov`), we see this precision comes at
        the cost of accuracy (it under-covers the parameter).
    -   **Method 1 (Exact):** produces moderately sized intervals (Len =
        2.77 at $\alpha=0.1$). It is wider than Method 2 but necessary
        to maintain correct coverage.
    -   **Method 3 (Normal, Solved):** Produces the widest (least
        precise) intervals at small sample sizes. At $\alpha=0.01$ and
        $n=10$, the length explodes to **11.12** compared to \~5.0 for
        Method 1. This is caused by the denominator $(1 - z/\sqrt{n})$
        becoming very small when $n$ is small.

**Recommendation:** **Method 1 (Exact Distribution using Chi-Squared)**
is the best method.

**Explanation:**

1.  **Validity (Coverage Probability):** Method 1 maintains the target
    confidence level ($1-\alpha$) correctly across **all** sample sizes.

    -   Looking at $n=10$ and $\alpha=0.05$ (Target 0.95), Method 1
        achieves **0.9464**.
    -   In contrast, **Method 4 (Student's t)** fails significantly at
        small sample sizes, achieving only **0.8949** coverage when the
        target is 0.95. This is because the underlying Exponential
        distribution is highly skewed, violating the normality
        assumption required for the Student's t-distribution at small
        $n$.

2.  **Efficiency vs. Method 3:** While **Method 3** (Normal Solved) is a
    valid asymptotic approach that does not require knowing $\theta$, it
    is mathematically unstable at small sample sizes. As noted in part
    (b), its interval length at $n=10$ is excessively large (Len \> 11
    at $\alpha=0.01$). Method 1 provides a much tighter, more useful
    interval while maintaining perfect coverage.

3.  **Feasibility vs. Method 2:** **Method 2** requires knowing the true
    variance (which depends on $\theta$). Since $\theta$ is what we are
    trying to estimate, Method 2 cannot be used in real-world problems.

## Task 2

The Poisson distribution $P(\theta)$ has mean $E[X] = \theta$ and
variance $Var[X] = \theta$. Our team id number is 23.\
We aim to evaluate different methods of constructing confidence
intervals for the parameter Œ∏ of a Poisson distribution. The key
questions are:\
1)How well do different CI methods capture the true parameter ùúÉ with the
prescribed confidence level 1-a?\
2)How do the lengths of the intervals compare in terms of precision?\
3)Which method is the most practical and reliable for
Poisson-distributed data?\
Our main question is how well do the three proposed confidence interval
constructions perform in capturing the true parameter $\theta$ for
samples drawn from a Poisson distribution?\
We will assess this by estimating the coverage probability (the fraction
of times the interval contains $\theta$) and the precision (the average
length of the interval) for different sample sizes $n$.\

1.Known Variance: Assuming $\theta$ is known for the variance
$\sigma^2 = \theta$.The $1-\alpha$ confidence interval for $\theta$ is
derived from
$|Z| = |\frac{\sqrt{n}(\bar{X}_n - \theta)}{\sqrt{\theta}}| \le z_{\alpha/2}$,
where $z_{\alpha/2}$ is the $(1-\alpha/2)$-th quantile of
$N(0, 1)$.$$\left[\bar{X}_n - z_{\alpha/2}\frac{\sqrt{\theta}}{\sqrt{n}}, \quad \bar{X}_n + z_{\alpha/2}\frac{\sqrt{\theta}}{\sqrt{n}}\right]$$\
2.Parameter-Independent: From Part (2), the defining inequality is
$\frac{n(\bar{X}_n - \theta)^2}{\theta} \le z_{\alpha/2}^2$. This is a
quadratic inequality in $\sqrt{\theta}$. Solving
$\frac{n(\bar{X}_n - \theta)^2}{\theta} = z_{\alpha/2}^2$ for $\theta$
yields the
limits:$$\theta_{\pm} = \bar{X}_n + \frac{z_{\alpha/2}^2}{2n} \pm \frac{z_{\alpha/2}}{\sqrt{n}} \sqrt{\bar{X}_n + \frac{z_{\alpha/2}^2}{4n}}$$The
$1-\alpha$ confidence interval
is:$$\left[\bar{X}_n + \frac{z_{\alpha/2}^2}{2n} - \frac{z_{\alpha/2}}{\sqrt{n}} \sqrt{\bar{X}_n + \frac{z_{\alpha/2}^2}{4n}}, \quad \bar{X}_n + \frac{z_{\alpha/2}^2}{2n} + \frac{z_{\alpha/2}}{\sqrt{n}} \sqrt{\bar{X}_n + \frac{z_{\alpha/2}^2}{4n}}\right]$$
3.

```{r}
id <- 23
set.seed(id)
theta <- id/10
M <- 10000
N_small <- 50
N_large <- 200
alpha_vals <- c(0.1, 0.05, 0.01)

run_simulation <- function(n, theta, M, alpha_vals) {
    x <- matrix(rpois(n * M, lambda = theta), nrow = n)

    sample_mean <- colMeans(x)

    results <- list()
    for (alpha in alpha_vals) {
        z_alpha2 <- qnorm(1 - alpha / 2)
        
        # --- Part (2): Theoretical CI (known variance theta, for comparison) ---
        # CI_2: [X_bar - z_a/2 * sqrt(theta/n), X_bar + z_a/2 * sqrt(theta/n)]
        L2 <- sample_mean - z_alpha2 * sqrt(theta / n)
        U2 <- sample_mean + z_alpha2 * sqrt(theta / n)
        coverage2 <- mean(theta >= L2 & theta <= U2)
        length2 <- 2 * z_alpha2 * sqrt(theta / n) # CI length is constant here
        
        # --- Part (3): Parameter-Independent CI (Solving quadratic) ---
        # CI_3: [X_bar + z^2/(2n) +/- (z/sqrt(n)) * sqrt(X_bar + z^2/(4n))]
        a <- z_alpha2^2 / (2 * n)
        b <- (z_alpha2 / sqrt(n))
        
        L3 <- sample_mean + a - b * sqrt(sample_mean + a / 2) # Note: a/2 = z^2/(4n)
        U3 <- sample_mean + a + b * sqrt(sample_mean + a / 2)
        
        coverage3 <- mean(theta >= L3 & theta <= U3)
        length3 <- U3 - L3 # CI length is variable
        
        # --- Part (4): Estimated Variance (Wald/Plug-in CI) ---
        # CI_4: [X_bar - z_a/2 * sqrt(X_bar/n), X_bar + z_a/2 * sqrt(X_bar/n)]
        # Note: We cap X_bar at a tiny positive value to avoid sqrt(0) error if all samples are 0.
        sample_sd <- apply(x, 2, sd)       # sample SD for each simulation
        se <- sample_sd / sqrt(n)          # standard error of the mean
        t_alpha2 <- qt(1 - alpha/2, df = n-1)  # t quantile

        L4 <- sample_mean - t_alpha2 * se
        U4 <- sample_mean + t_alpha2 * se

        coverage4 <- mean(theta >= L4 & theta <= U4)
        length4 <- U4 - L4
        
        results[[paste0("alpha_", alpha)]] <- list(
            Coverage = c(CI2=coverage2, CI3=coverage3, CI4=coverage4),
            Avg_Length = c(CI2=mean(length2), CI3=mean(length3), CI4=mean(length4))
        )
    }
    return(results)
}

results_N50 <- run_simulation(N_small, theta, M, alpha_vals)
results_N200 <- run_simulation(N_large, theta, M, alpha_vals)

# --- Output the results ---
cat("\n## üìä Simulation Results (theta = 2.3) ##\n")
cat("\n### üî¨ Sample Size n = 50: Coverage Probability ###\n")
print(do.call(rbind, lapply(results_N50, function(x) x$Coverage)))

cat("\n### üî¨ Sample Size n = 50: Average CI Length ###\n")
print(do.call(rbind, lapply(results_N50, function(x) x$Avg_Length)))

cat("\n### üî¨ Sample Size n = 200: Coverage Probability ###\n")
print(do.call(rbind, lapply(results_N200, function(x) x$Coverage)))

cat("\n### üî¨ Sample Size n = 200: Average CI Length ###\n")
print(do.call(rbind, lapply(results_N200, function(x) x$Avg_Length)))

# --- Plotting the sample means (Illustrative Statistics) ---
cat("\n## üìà Illustrative Statistics and Histograms ##\n")
cat(paste("True Parameter theta:", theta, "\n"))
# Note: Re-running the simulation setup to get fresh data for descriptive stats
x_n50_all <- matrix(rpois(N_small * M, lambda = theta), nrow = N_small)
x_n200_all <- matrix(rpois(N_large * M, lambda = theta), nrow = N_large)
cat(paste("Mean of Sample Means (n=50):", mean(colMeans(x_n50_all)), "\n"))
cat(paste("Mean of Sample Means (n=200):", mean(colMeans(x_n200_all)), "\n"))

# Generate sample means for plotting
x_n50 <- colMeans(x_n50_all)
x_n200 <- colMeans(x_n200_all)
```

### Justification

The foundation of all three confidence intervals is the Central Limit
Theorem
(CLT).$$\frac{\sqrt{n}(\bar{X}_n - \theta)}{\sigma} \stackrel{d}{\to} N(0, 1) \quad \text{as } n \to \infty$$For
the Poisson distribution $P(\theta)$, the standard deviation is
$\sigma = \sqrt{\theta}$.Therefore, the statistic
$Z = \frac{\sqrt{n}(\bar{X}_n - \theta)}{\sqrt{\theta}}$ is
approximately standard normal $N(0, 1)$ for large $n$.\
1)2nd method directly uses the statistic $Z$ and the approximation
$P(|Z| \le z_{\alpha/2}) \approx 1-\alpha$. The coverage is expected to
be close to $1-\alpha$ due to the CLT. The simulation confirms this,
with coverages very close to the target.\
2)4rd method liminates Œ∏ from the CI by solving a quadratic inequality;
this ensures coverage without knowing the true variance.\
3)4th method Uses the sample standard deviation and the t-distribution,
making it practical for real-world scenarios where Œ∏ is unknown.

### Conclusion

# Part II: Unbiasedness of Estimators

## Task 3

```{r}
set.seed(42)
n <- 100
mu <- 10
sigma_squared <- 4
sigma <- sqrt(sigma_squared)
dataset <- rnorm(n, mean = mu, sd = sigma)
head(dataset)
## [1] 12.741917 8.870604 10.726257 11.265725 10.808537 9.787751
cat("Population Mean (mu):", mu, "\n")
## Population Mean (mu): 10
cat("Population Variance (sigma_squared):", sigma_squared, "\n")
## Population Variance (sigma_squared): 4
sample_mean <- mean(dataset)
sample_variance <- var(dataset)
cat("Sample Mean:", sample_mean, "\n")
## Sample Mean: 10.06503
cat("Sample Variance:", sample_variance, "\n")
## Sample Variance: 4.337697
```

## Task a)

```{r}

sigma_n_sq_single <- (1/n) * sum((dataset - sample_mean)^2)
sigma_n_minus_1_sq_single <- (1/(n-1)) * sum((dataset - sample_mean)^2)

cat("Task a) \n")
cat(paste("True Population Variance (sigma^2):", sigma_squared, "\n"))
cat(paste("Biased Estimator (sigma_n^2):", sigma_n_sq_single, "\n"))
cat(paste("Unbiased Estimator (sigma_n-1^2):", sigma_n_minus_1_sq_single, "\n"))
```

## Task b)

```{r}


N_vals <- c(10, 50, 100, 1000)

results_b <- data.frame(
    n = integer(),
    sigma_n_sq = numeric(),
    sigma_n_minus_1_sq = numeric()
)

for (current_n in N_vals) {
    current_dataset <- rnorm(current_n, mean = mu, sd = sigma)

    sum_sq_dev <- sum((current_dataset - current_sample_mean)^2)

    # Calculate Biased Estimator (sigma_n^2)
    sigma_n_sq_calc <- sum_sq_dev / current_n

    # Calculate Unbiased Estimator (sigma_n-1^2)
    sigma_n_minus_1_sq_calc <- sum_sq_dev / (current_n - 1)

    results_b <- rbind(results_b, data.frame(
        n = current_n,
        sigma_n_sq = sigma_n_sq_calc,
        sigma_n_minus_1_sq = sigma_n_minus_1_sq_calc
    ))
}
print(results_b)

# In part b) sigma_n_sq is closer to theoretical sigma_sq when the sample variance > theoretical one.
# That`s because sigma_n_sq is always less than sigma_n_minus_1_sq
```

## Task c)

```{r}


run_bias_simulation <- function(n, mu, sigma_squared, M) {
    samples_matrix <- matrix(rnorm(n * M, mean = mu, sd = sqrt(sigma_squared)), nrow = n)
    sample_means <- colMeans(samples_matrix)

    # Calculate Sum of Squared Deviations (SSD)
    deviations <- samples_matrix - matrix(sample_means, nrow = n, ncol = M, byrow = TRUE)
    sum_sq_dev <- colSums(deviations^2)

    # Biased Estimator (divides by n)
    sigma_n_sq_vals <- sum_sq_dev / n
    # Unbiased Estimator (divides by n-1)
    sigma_n_minus_1_sq_vals <- sum_sq_dev / (n - 1)

    # Calculate Expected Values (Average over M simulations)
    E_sigma_n_sq <- mean(sigma_n_sq_vals)
    E_sigma_n_minus_1_sq <- mean(sigma_n_minus_1_sq_vals)

    # Bias = Expected Value - True Parameter
    Bias_n <- E_sigma_n_sq - sigma_squared
    Bias_n_minus_1 <- E_sigma_n_minus_1_sq - sigma_squared
    return(data.frame(
        n = n,
        True_Var = sigma_squared,
        E_sigma_n_sq = E_sigma_n_sq,
        Bias_sigma_n_sq = Bias_n,
        E_sigma_n_minus_1_sq = E_sigma_n_minus_1_sq,
        Bias_sigma_n_minus_1_sq = Bias_n_minus_1
    ))
}

M_sim <- 10000

results_list <- lapply(N_vals, run_bias_simulation, mu=mu, sigma_squared=sigma_squared, M=M_sim)
final_results_c <- do.call(rbind, results_list)

print(final_results_c)

```

## Task d)

We observe different behaviours of estimators:

The Unbiased Estimator (sigma_n-1\^2): The expected value
E[sigma_n-1\^2] remains consistently close to the true population
variance (sigma\^2 = 4) for all sample sizes (n=10, 50, 100, 1000). The
calculated bias is small regardless of n. This confirms that
sigma_n-1\^2 is an unbiased estimator.

The Biased Estimator (sigma_n\^2): For small sample sizes (e.g., n=10),
this estimator significantly underestimates the true variance
(E[sigma_10\^2] is approx 3.6, which is 10% lower than 4). As n
increases, the underestimation becomes smaller. At n=10, the bias is
large (approx -0.4), but at n=1000, the bias is very small (approx
-0.004), making the estimate nearly identical to the unbiased one.

Mathematical Reason: This happens because the ratio (n-1)/n approaches 1
as n approaches infinity. Therefore, the limit of E[sigma_n\^2] as n
approaches infinity equals sigma\^2.

Conclusion: While sigma_n\^2 is biased for finite samples, it is
asymptotically unbiased. The difference between the two estimators is
critical for small datasets but becomes small for very large datasets.
For statistics sigma_n-1\^2 is always preferred.

## Task e)

We aim to derive the expected values for both estimators:
$$\sigma_n^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 \quad \text{and} \quad \sigma_{n-1}^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2$$

First, let us analyze the expected value of the sum of squared
deviations: $E\left[ \sum_{i=1}^n (X_i - \overline{X})^2 \right]$.

We add and subtract the true population mean $\mu$ inside the summation:
$$\sum_{i=1}^n (X_i - \overline{X})^2 = \sum_{i=1}^n ((X_i - \mu) - (\overline{X} - \mu))^2$$

Expanding the squared term $(a-b)^2 = a^2 - 2ab + b^2$:
$$= \sum_{i=1}^n \left( (X_i - \mu)^2 - 2(X_i - \mu)(\overline{X} - \mu) + (\overline{X} - \mu)^2 \right)$$

Distributing the summation:
$$= \sum_{i=1}^n (X_i - \mu)^2 - 2(\overline{X} - \mu) \sum_{i=1}^n (X_i - \mu) + \sum_{i=1}^n (\overline{X} - \mu)^2$$

Note that $\sum_{i=1}^n (X_i - \mu) = n(\overline{X} - \mu)$ and
$\sum_{i=1}^n (\overline{X} - \mu)^2 = n(\overline{X} - \mu)^2$.
Substituting these back:
$$= \sum_{i=1}^n (X_i - \mu)^2 - 2n(\overline{X} - \mu)^2 + n(\overline{X} - \mu)^2$$
$$= \sum_{i=1}^n (X_i - \mu)^2 - n(\overline{X} - \mu)^2$$

**Applying Expectation** Now we take the expected value of the
simplified expression:
$$E\left[ \sum_{i=1}^n (X_i - \overline{X})^2 \right] = \sum_{i=1}^n E[(X_i - \mu)^2] - n E[(\overline{X} - \mu)^2]$$

Recall the definitions of population variance and variance of the sample
mean: 1. $E[(X_i - \mu)^2] = Var(X) = \sigma^2$ 2.
$E[(\overline{X} - \mu)^2] = Var(\overline{X}) = \frac{\sigma^2}{n}$

Substituting these values:
$$= \sum_{i=1}^n \sigma^2 - n \left( \frac{\sigma^2}{n} \right)$$
$$= n\sigma^2 - \sigma^2$$ $$= (n-1)\sigma^2$$

**Expected Values of the Estimators**

1.  For the **Biased Estimator** ($\sigma_n^2$):
    $$E[\sigma_n^2] = E\left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 \right] = \frac{1}{n} (n-1)\sigma^2 = \frac{n-1}{n}\sigma^2$$
    Since $E[\sigma_n^2] \neq \sigma^2$, this estimator is biased.

2.  For the **Unbiased Estimator** ($\sigma_{n-1}^2$):
    $$E[\sigma_{n-1}^2] = E\left[ \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2 \right] = \frac{1}{n-1} (n-1)\sigma^2 = \sigma^2$$
    Since $E[\sigma_{n-1}^2] = \sigma^2$, this estimator is unbiased.

## Task f)

An estimator $\hat{\theta}$ is defined as unbiased if its expected value
equals the true parameter $\theta$:

$$E[\hat{\theta}] = \theta \quad \iff \quad Bias(\hat{\theta}) = E[\hat{\theta}] - \theta = 0$$ 1.
Testing the Estimator $\sigma_n^2$

From part (e), we have:

$$E[\sigma_n^2] = \frac{n-1}{n}\sigma^2$$ Calculating the Bias:

$$Bias(\sigma_n^2) = E[\sigma_n^2] - \sigma^2 = \frac{n-1}{n}\sigma^2 - \sigma^2 = \sigma^2 \left( \frac{n-1}{n} - 1 \right) = \sigma^2 \left( \frac{n-1 - n}{n} \right) = \sigma^2 \left( \frac{-1}{n} \right) = -\frac{\sigma^2}{n} $$

Since $Bias(\sigma_n^2) \neq 0$, the estimator $\sigma_n^2$ is biased
(it systematically underestimates the variance).

2.  Testing the Estimator $\sigma_{n-1}^2$

From part (e), we have:

$$E[\sigma_{n-1}^2] = \sigma^2$$

Calculating the Bias:

$$Bias(\sigma_{n-1}^2) = E[\sigma_{n-1}^2] - \sigma^2 = \sigma^2 - \sigma^2 = 0$$
Since $Bias(\sigma_{n-1}^2) = 0$, the estimator $\sigma_{n-1}^2$ is
unbiased.

## Task g)

The results from our task c) are in perfect agreement with the
theoretical derivations (Tasks e and f).

Theory: we proved mathematically that $E[\sigma_{n-1}^2] = \sigma^2$ and
$E[\sigma_n^2] = \frac{n-1}{n}\sigma^2$.

Practice: The simulation showed that the average of $\sigma_{n-1}^2$
over 10,000 runs was almost exactly 4 (the true variance), while the
average of $\sigma_n^2$ was consistently lower (e.g., $\approx 3.6$ for
$n=10$), matching the theoretical prediction of systematic
underestimation.

The fundamental reason $\sigma_n^2$ is biased is that it treats the
sample mean $\overline{X}$ as if it was the true population mean $\mu$.
However, $\overline{X}$ is calculated from the data itself, which
minimizes the sum of squared deviations for that specific sample. This
makes the deviations $(X_i - \overline{X})^2$ slightly smaller on
average than $(X_i - \mu)^2$. Dividing by $n-1$ (degrees of freedom)
instead of $n$ perfectly compensates for this.

Practical Implication: Small Samples: For small datasets (e.g.,
$n < 30$), using the biased estimator $\sigma_n^2$ leads to a
significant error. In these cases, using $\sigma_{n-1}^2$ is critical.

Large Samples: As $n \to \infty$, the difference between $\frac{1}{n}$
and $\frac{1}{n-1}$ becomes negligible. Both estimators converge to the
true parameter, making $\sigma_n^2$ asymptotically unbiased.
